Decision tree regression uses a decision tree (as a predictive model) to go from observations about an item to conclusions about the item's target value.<br>

It is one of the predictive modelling approaches used in statistics, data mining and machine learning.<br>

What is Decision Tree?<br>

A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.<br>
It is one way to display an algorithm.<br>

Goal:<br>

The goal is to create a model that predicts the value of a target variable based on several input variables.<br>
An example is shown in the diagram.<br>

<img src = "/images/models/decisionTreeRegression.png">

Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable.<br>
Each leaf represents a value of the target variable given the values of the input variables represented by the path from the root to the leaf.<br>

What is this doing?<br>

The decision trees is used to fit a sine curve with addition noisy observation.<br>
As a result, it learns local linear regressions approximating the sine curve.<br>
We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.<br>
Decision tree regression uses a decision tree (as a predictive model) to go from observations about an item to conclusions about the item's target value.
<br>
It is one of the predictive modelling approaches used in statistics, data mining and machine learning.
<br>
What is Decision Tree?
<br>
A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.
It is one way to display an algorithm.
<br>
Goal:
<br>
The goal is to create a model that predicts the value of a target variable based on several input variables.<br>
An example is shown in the diagram.
<br>
<img src = "images/models/decisionTreeRegression.png">
<br>
Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable.
Each leaf represents a value of the target variable given the values of the input variables represented by the path from the root to the leaf.
<br>
<br>
What is this doing?
<br>
The decision trees is used to fit a sine curve with addition noisy observation.
As a result, it learns local linear regressions approximating the sine curve.
We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.

